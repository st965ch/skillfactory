{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b663768-1f81-40e9-99f2-8a188cad967a",
   "metadata": {},
   "source": [
    "[SF-DST] Booking reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb259a-f695-4010-833f-bbfe4add0bf5",
   "metadata": {},
   "source": [
    "Описание:\n",
    "Представьте, что вы работаете датасаентистом в компании Booking. Одна из проблем компании — это нечестные отели, которые накручивают себе рейтинг. Одним из способов нахождения таких отелей является построение модели, которая предсказывает рейтинг отеля. Если предсказания модели сильно отличаются от фактического результата, то, возможно, отель играет нечестно, и его стоит проверить. Вам поставлена задача создать такую модель."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde74740-9787-471d-aa80-d1356d47084c",
   "metadata": {},
   "source": [
    "Описание:\n",
    "Представьте, что вы работаете датасаентистом в компании Booking. Одна из проблем компании — это нечестные отели, которые накручивают себе рейтинг. Одним из способов нахождения таких отелей является построение модели, которая предсказывает рейтинг отеля. Если предсказания модели сильно отличаются от фактического результата, то, возможно, отель играет нечестно, и его стоит проверить. Вам поставлена задача создать такую модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef55b5c1-7a12-4946-82c9-54f9678c4224",
   "metadata": {},
   "outputs": [],
   "source": [
    "Подготовка рабочего пространства"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a89992-7877-40df-8888-158036c77274",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = '/content/'\n",
    "df_train = pd.read_csv(DATA_DIR+'hotels_train.csv')               # датасет для обучения\n",
    "df_test = pd.read_csv(DATA_DIR+'hotels_test.csv')                 # датасет для предсказания\n",
    "#sample_submission = pd.read_csv(DATA_DIR+'/submission.csv')      # самбмишн Сабмишн (англ. submission) — это победа в поединке по единоборствам,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4655dd8e-a97d-4e55-b605-400e6877322c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# DATA_DIR = '/content/'\n",
    "df_train = pd.read_csv('hotels_train1.csv')               # датасет для обучения\n",
    "df_test = pd.read_csv('hotels_test1.csv')                 # датасет для предсказания\n",
    "#sample_submission = pd.read_csv(DATA_DIR+'/submission.csv')      # самбмишн"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ef467-14ce-4887-bb9d-d26ea4fbd932",
   "metadata": {},
   "source": [
    "Столбец с названием reviewer_score обычно представляет собой числовой показатель оценки, выставленной рецензентом (например, оценка фильма, продукта, услуги и т.п.). Это может быть рейтинг или балл, отражающий мнение или качество с точки зрения обозревателя.\n",
    "В зависимости от контекста данных reviewer_score может содержать:\n",
    "Числовые значения (например, от 0 до 10 или 0 до 100),\n",
    "Баллы или оценки, выставленные человеком или системой,\n",
    "Количественную меру качества или удовлетворённости.\n",
    "Часто такой столбец используется для анализа или фильтрации данных по значению оценки.\n",
    "Если речь о конкретном DataFrame или наборе данных, reviewer_score — это колонка с числовыми оценками или рейтингами, которые можно использовать для аналитики, сортировки, вычисления средних значений и других статистических операций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06c94417-ca1e-432c-aea3-b7d1f0e3d94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# В pandas метод sample() используется для получения случайной выборки из строк DataFrame.\n",
    "# Это означает, что с помощью sample() можно:\n",
    "# Выбрать случайное количество строк (например, 5 случайных строк).\n",
    "# Выбрать случайную часть DataFrame, указав долю (например, 10% строк).\n",
    "# Перемешать строки всего DataFrame, если взять frac=1 (100% строк в случайном порядке).\n",
    "# Пример использования:\n",
    "# df.sample(n=5)       # 5 случайных строк из df\n",
    "# df.sample(frac=0.1)  # 10% строк случайно из df\n",
    "# df.sample(frac=1)    # перемешать все строки df\n",
    "df_train['sample'] = 1           # помечаем где у нас трейн\n",
    "df_test['sample'] = 0            # помечаем где у нас тест\n",
    "df_test['reviewer_score'] = 0    # в тесте у нас нет значения reviewer_score, мы его должны предсказать, поэтому пока просто заполняем нулями ( reviewer_score обычно представляет собой числовой показатель оценки)\n",
    "reviews_df = pd.concat([df_test, df_train], ignore_index=True, sort=False)\n",
    "# reviews_df = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем train Для объединения df_test и df_train с помощью pd.concat() вместо устаревшего append()\n",
    "# Объяснение изменений:\n",
    "# pd.concat() принимает список DataFrame для объединения.\n",
    "# Параметр ignore_index=True сбрасывает индексы в итоговом DataFrame.\n",
    "# Параметр sort=False предотвращает автоматическую сортировку столбцов, оставляя порядок неизменным.\n",
    "# Таким образом, код с concat() полностью заменит строку с append() и reset_index() за один вызов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe49628-cc0f-40b2-9758-aa125ac75b3f",
   "metadata": {},
   "source": [
    "df = df.append(new_row, ignore_index=True) \n",
    "Ошибка 'DataFrame' object has no attribute 'append' возникает потому, что метод append() для объектов pandas DataFrame был устаревшим начиная с версии 1.4.0, а в версии Pandas 2.0 был полностью удалён. Этот метод раньше использовался для добавления строк к DataFrame, но теперь он больше не поддерживается.\n",
    "df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "Таким образом, ошибка означает, что версия Pandas современная (2.0 и новее), где метода append() уже нет, и нужно заменить этот вызов на concat() для правильного добавления строк или объединения DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b334dc2-aae9-4a08-a339-760c38b1dd5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 515738 entries, 0 to 515737\n",
      "Data columns (total 18 columns):\n",
      " #   Column                                      Non-Null Count   Dtype  \n",
      "---  ------                                      --------------   -----  \n",
      " 0   hotel_address                               515738 non-null  object \n",
      " 1   additional_number_of_scoring                515738 non-null  int64  \n",
      " 2   review_date                                 515738 non-null  object \n",
      " 3   average_score                               515738 non-null  float64\n",
      " 4   hotel_name                                  515738 non-null  object \n",
      " 5   reviewer_nationality                        515738 non-null  object \n",
      " 6   negative_review                             515738 non-null  object \n",
      " 7   review_total_negative_word_counts           515738 non-null  int64  \n",
      " 8   total_number_of_reviews                     515738 non-null  int64  \n",
      " 9   positive_review                             515738 non-null  object \n",
      " 10  review_total_positive_word_counts           515738 non-null  int64  \n",
      " 11  total_number_of_reviews_reviewer_has_given  515738 non-null  int64  \n",
      " 12  tags                                        515738 non-null  object \n",
      " 13  days_since_review                           515738 non-null  object \n",
      " 14  lat                                         512470 non-null  float64\n",
      " 15  lng                                         512470 non-null  float64\n",
      " 16  sample                                      515738 non-null  int64  \n",
      " 17  reviewer_score                              515738 non-null  float64\n",
      "dtypes: float64(4), int64(6), object(8)\n",
      "memory usage: 70.8+ MB\n"
     ]
    }
   ],
   "source": [
    "reviews_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dc0b437-11b8-47bc-972b-830d2f8a1010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>is_bad_review</th>\n",
       "      <th>reviewer_score</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Would have appreciated a shop in the hotel th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No tissue paper box was present at the roomNo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pillows  Nice welcoming and service</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Negative Everything including the nice upgr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Negative Lovely hotel v welcoming staff</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  is_bad_review  \\\n",
       "0   Would have appreciated a shop in the hotel th...              1   \n",
       "1   No tissue paper box was present at the roomNo...              1   \n",
       "2                Pillows  Nice welcoming and service              1   \n",
       "3  No Negative Everything including the nice upgr...              1   \n",
       "4        No Negative Lovely hotel v welcoming staff               1   \n",
       "\n",
       "   reviewer_score  sample  \n",
       "0             0.0       0  \n",
       "1             0.0       0  \n",
       "2             0.0       0  \n",
       "3             0.0       0  \n",
       "4             0.0       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# объединяем положительные и отрицательные отзывы\n",
    "reviews_df[\"review\"] = reviews_df[\"negative_review\"] + reviews_df[\"positive_review\"]\n",
    "# создаём метку для плохого отзыва (1 - True, 0 - False)\n",
    "reviews_df[\"is_bad_review\"] = reviews_df[\"reviewer_score\"].apply(lambda x: 1 if x < 5 else 0)\n",
    "# выбираем только релевантные столбцы\n",
    "reviews_df = reviews_df[[\"review\", \"is_bad_review\", \"reviewer_score\", \"sample\"]]\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e84208-8a2d-462d-9ca1-eae7c1013e47",
   "metadata": {},
   "source": [
    "Sample данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88085d0b-7807-482e-915a-c26a1e91d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = reviews_df.sample(frac = 0.1, replace = False, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1af5595d-1250-41aa-b4a0-7ea292da3e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# удалим 'No Negative' или 'No Positive' из текста\n",
    "reviews_df[\"review\"] = reviews_df[\"review\"].apply(lambda x: x.replace(\"No Negative\", \"\").replace(\"No Positive\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab9291-487c-487b-a2b3-be834327a449",
   "metadata": {},
   "source": [
    "- import nltk — это команда в Python, которая импортирует библиотеку NLTK (Natural Language Toolkit).\n",
    "NLTK — одна из самых популярных и мощных библиотек для обработки естественного языка (Natural Language Processing, NLP) в Python. Она предоставляет множество инструментов и ресурсов для работы с текстом: токенизацию, стемминг, лемматизацию, разметку частей речи, синтаксический разбор, корпуса и словари, а также алгоритмы для анализа текста и машинного обучения.\n",
    "Эту библиотеку используют, когда надо анализировать, обрабатывать и понимать человеческий язык в программных приложениях, например, для создания чат-ботов, анализа тональности отзывов, автоматического перевода и других задач NLP.\n",
    "\n",
    "- from nltk.corpus import wordnet означает импорт из библиотеки NLTK модуля WordNet.\n",
    "  \n",
    "- nltk.corpus — это модуль библиотеки NLTK, который предоставляет доступ к разнообразным текстовым корпусам и словарным ресурсам для обработки естественного языка (NLP) в Python.\n",
    "  \n",
    "- WordNet — это большая лексическая база данных английского языка, представляющая собой своего рода электронный словарь и тезаурус. В ней слова объединены в синонимические группы (синсеты), каждая из которых имеет определение, примеры употребления, а также связи с другими словами (синонимы, антонимы и другие отношения).\n",
    "NLTK предоставляет удобный интерфейс для работы с WordNet: можно получать определения слов, списки синонимов, примеры использования, искать антонимы, строить семантические сети и проводить другие лингвистические анализы.\n",
    "\n",
    "- Лемматизация — это процесс приведения слова к его базовой, начальной форме, называемой леммой. Например, слова \"бежал\", \"бегу\" и \"бежать\" после лемматизации превратятся в одну лемму — \"бежать\". Этот метод используется в обработке естественного языка (NLP) и поисковых системах для того, чтобы компьютер мог \"понимать\" разные грамматические формы одного слова как единое целое, что повышает точность анализа текста. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "535cf251-4259-4a33-a672-78bc5086ab5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Станислав\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Станислав\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Станислав\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# https://webdevblog.ru/podhody-lemmatizacii-s-primerami-v-python/\n",
    "# использование wordnet для анализа частей речи\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# используется для загрузки в библиотеку NLTK предобученной модели для частеречной разметки текста — POS-теггера (Part-of-Speech tagger), \n",
    "# основанного на алгоритме усреднённого персептрона (averaged perceptron).\n",
    "\n",
    "nltk.download('wordnet')\n",
    "# означает загрузку в NLTK лексической базы данных WordNet.\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# загружает в NLTK набор списков стоп-слов для разных языков.\n",
    "# Стоп-слова — это часто встречающиеся в языке слова (например, предлоги, союзы, частицы: «и», «в», «не», «что»), \n",
    "# которые обычно не несут важной смысловой нагрузки и часто удаляются из текста при его обработке и анализе.\n",
    "# NLTK не включает эти списки автоматически, поэтому их нужно скачать отдельно с помощью nltk.download('stopwords').\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "# Функция get_wordnet_pos(pos_tag) обычно используется в связке с библиотекой NLTK для корректной лемматизации слов с учётом их части речи (POS — Part Of Speech).\n",
    "# Что делает эта функция:\n",
    "# Принимает POS-тег в формате Penn Treebank (формат, который выдаёт функция nltk.pos_tag),\n",
    "# Преобразует этот тег к формату POS-тегов, который ожидает лемматизатор WordNet из NLTK (wordnet.NOUN, wordnet.VERB, wordnet.ADJ, wordnet.ADV),\n",
    "# Если тег не подходит под известные категории, возвращает wordnet.NOUN по умолчанию.\n",
    "    \n",
    "    if pos_tag.startswith('J'):\n",
    "# означает проверку, начинается ли строка pos_tag с буквы 'J'.\n",
    "# В контексте NLTK и POS-тегов (частей речи) это важно, потому что теги POS имеют формат Penn Treebank, где:\n",
    "# Теги, начинающиеся с \"J\" обозначают прилагательные (adjectives).\n",
    "# Например, \"JJ\" — прилагательное, \"JJR\" — прилагательное в сравнительной степени, \"JJS\" — прилагательное в превосходной степени.\n",
    "# Поэтому эта проверка используется, чтобы определить, относится ли тег к прилагательным.\n",
    "        \n",
    "        return wordnet.ADJ\n",
    "# это возвращение из функции значения wordnet.ADJ, означающего часть речи прилагательное (adjective) в формате, который понимает лемматизатор WordNet из библиотеки NLTK.\n",
    "# В NLTK для лемматизации нужной является информация о части речи слова (POS). wordnet.ADJ — специальная константа, указывающая, что слово является прилагательным. \n",
    "# Это помогает лемматизатору правильно найти базовую форму слова с учётом его грамматической категории.\n",
    "    \n",
    "    elif pos_tag.startswith('V'):\n",
    "# проверяет, начинается ли POS-тег (часть речи), заданный в формате Penn Treebank, с буквы \"V\".\n",
    "# В системе тегов Penn Treebank все теги, начинающиеся на \"V\", обозначают глаголы.\n",
    "# Примеры таких тегов:\n",
    "# VB — глагол в базовой форме (например, \"run\"),\n",
    "# VBD — глагол в прошедшем времени (\"ran\"),\n",
    "# VBG — герундий или причастие настоящего времени (\"running\"),\n",
    "# VBN — причастие прошедшего времени (\"run\"),\n",
    "# VBP — глагол в настоящем времени (не третье лицо, \"run\"),\n",
    "# VBZ — глагол в настоящем времени третьего лица единственного числа (\"runs\").\n",
    "# Поэтому эта проверка используется для определения, что слово в предложении является глаголом, что позволяет, например, \n",
    "# корректно сопоставить тег с соответствующей частью речи для лемматизации или другой обработки языка.\n",
    "        \n",
    "        return wordnet.VERB\n",
    "# это возвращение из функции значения wordnet.VERB, которое обозначает часть речи глагол (verb) в формате, который понимает лемматизатор WordNet из библиотеки NLTK.\n",
    "# Это необходимо для лемматизации, при которой важно знать, к какой части речи относится слово, чтобы правильно привести его к базовой форме. wordnet.VERB указывает, \n",
    "# что слово является глаголом.\n",
    "\n",
    "    elif pos_tag.startswith('N'):\n",
    "# проверяет, начинается ли POS-тег, заданный в формате Penn Treebank, с буквы \"N\".\n",
    "# В системе тегов Penn Treebank теги, начинающиеся с \"N\", обозначают существительные (nouns).\n",
    "# Примеры таких тегов:\n",
    "# NN — существительное в единственном числе (например, \"cat\"),\n",
    "# NNS — существительное во множественном числе (например, \"cats\"),\n",
    "# NNP — собственное имя (например, \"John\"),\n",
    "# NNPS — собственные имена во множественном числе.\n",
    "# Таким образом, эта проверка используется, чтобы определить, что часть речи слова является существительным, что важно, например, \n",
    "# для правильной обработки слова в лемматизации или синтаксическом разборе\n",
    "\n",
    "        return wordnet.NOUN\n",
    "# это возвращение из функции специального обозначения части речи существительное (noun) в формате, используемом WordNet из библиотеки NLTK.\n",
    "# В контексте лемматизации WordNet требует указания части речи слова, чтобы правильно привести его к базовой форме. \n",
    "# wordnet.NOUN — это внутренний код, который обозначает, что слово является существительным.\n",
    "\n",
    "    elif pos_tag.startswith('R'):\n",
    "# проверяет, начинается ли POS-тег в формате Penn Treebank с буквы \"R\".\n",
    "# В системе тегов Penn Treebank теги, начинающиеся с \"R\", обозначают наречия (adverbs).\n",
    "# Примеры таких тегов:\n",
    "# RB — наречие (например, \"quickly\"),\n",
    "# RBR — сравнительная степень наречия (например, \"faster\"),\n",
    "# RBS — превосходная степень наречия (например, \"fastest\").\n",
    "# Таким образом, эта проверка нужна, чтобы определить, что часть речи слова — наречие, что важно для правильной лингвистической обработки, например, для лемматизации.\n",
    "\n",
    "        return wordnet.ADV\n",
    "# это возврат из функции значения wordnet.ADV, которое обозначает часть речи наречие (adverb) в формате, используемом лемматизатором WordNet библиотеки NLTK.\n",
    "# В процессе лемматизации важно указывать часть речи слова для правильного нахождения базовой формы. wordnet.ADV указывает, что слово — это наречие.\n",
    "\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "# это возвращение из функции специальной константы wordnet.NOUN, которая обозначает часть речи существительное (noun) в формате, понятном лемматизатору WordNet из библиотеки NLTK.\n",
    "# Это нужно для правильной лемматизации слов с учётом их категории. Указание, что слово — существительное, помогает алгоритму привести слово к базовой, словарной форме корректно.\n",
    "\n",
    "import string\n",
    "# импортирует встроенный в Python модуль string, который предоставляет удобные константы и функции для работы со строками.\n",
    "# Модуль string содержит, например:\n",
    "# Константы с предопределёнными наборами символов: ascii_letters (латинские буквы), digits (цифры), punctuation (знаки пунктуации) и др.\n",
    "# Полезные функции для обработки строк, например, capwords() — для капитализации первой буквы каждого слова.\n",
    "# Классы для форматирования и шаблонов, например Formatter и Template.\n",
    "# Этот модуль часто используется для упрощения операций с текстом, например, проверки, какие символы есть в строке, форматирования сообщений, создания шаблонов для подстановки значений.\n",
    "    \n",
    "from nltk import pos_tag\n",
    "# означает импорт функции pos_tag из библиотеки NLTK.\n",
    "# Функция pos_tag используется для автоматической разметки текста, то есть для присвоения каждому слову в предложении его части речи (POS — Part Of Speech), \n",
    "# например, существительное, глагол, прилагательное и т.д.\n",
    "# Процесс называется POS-тегингом (тегированием частей речи) и позволяет анализировать грамматическую структуру текста.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# означает импорт в программу набора списков стоп-слов из корпуса NLTK.\n",
    "# Стоп-слова — это часто встречающиеся слова в языке (например, предлоги, союзы, частицы: «и», «в», «не», «что»), которые обычно не несут смысла и могут удаляться из текста при обработке для уменьшения шума.\n",
    "# NLTK содержит готовые списки стоп-слов для разных языков, например, для русского, английского и других.\n",
    "# Импортируя stopwords, можно получить эти списки и использовать для фильтрации текста\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "# означает импорт класса WhitespaceTokenizer из библиотеки NLTK.\n",
    "# WhitespaceTokenizer — это простой токенизатор, который разбивает текст на токены (слова) по пробельным символам. Он разделяет текст на слова, используя пробелы, табуляции, \n",
    "# переводы строк и другие символы пробела как разделители.\n",
    "# В отличие от более сложных токенизаторов, он не учитывает знаки препинания и другие символы, просто разбивает текст по пробелам. Это делает его быстрым, но менее точным для сложного анализа.\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# означает импорт класса WordNetLemmatizer из библиотеки NLTK.\n",
    "# WordNetLemmatizer — это инструмент для лемматизации слов на английском языке. Лемматизация — это процесс приведения слова к его базовой или словарной форме (лемме) с учётом его части речи и контекста.\n",
    "# В отличие от стемминга, который просто обрезает окончания слов, лемматизация учитывает грамматические и смысловые особенности слова, поэтому результат точнее.\n",
    "# Например, слова \"studies\" и \"studying\" будут приведены к форме \"study\".\n",
    "\n",
    "def clean_text(text):\n",
    "# это объявление функции с именем clean_text, которая принимает один аргумент text. Обычно такая функция предназначена для очистки или предобработки текста.\n",
    "# Функция очистки текста может выполнять следующие задачи:\n",
    "# удаление лишних пробелов в начале и конце строки,\n",
    "# удаление или замена специальных символов и знаков препинания,\n",
    "# удаление стоп-слов,\n",
    "# нормализация регистра (например, преобразование всего текста к нижнему регистру),\n",
    "# удаление чисел, HTML-тегов, или других посторонних элементов,\n",
    "# замена сокращений,\n",
    "# и другие операции, упрощающие последующий анализ текста.\n",
    "    \n",
    "# приводим к нижнему регистру\n",
    "    text = text.lower()\n",
    "    \n",
    "# разделение текста на слова и удаление знаков препинания\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "# означает следующее:\n",
    "# text.split(\" \") разбивает строку text на список слов, разделяя по пробелам.\n",
    "# Для каждого слова из этого списка (for word in ...) выполняется операция word.strip(string.punctuation).\n",
    "# Метод strip(string.punctuation) удаляет все знаки пунктуации (например, запятые, точки, восклицательные знаки и т.п.) с начала и конца слова.\n",
    "# Результатом работы будет новый список слов без ведущих и завершающих знаков пунктуации.\n",
    "# То есть, эта строка кода реализует очистку текста от пунктуации, разбивая текст на слова и очищая каждое слово от начальных и конечных знаков препинания.\n",
    "    \n",
    "# удаляем цифры из текста\n",
    "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "# делает следующее:\n",
    "# Итерация по каждому слову в списке text.\n",
    "# Для каждого слова проверяется условие: если в слове нет ни одной цифры (not any(c.isdigit() for c in word)).\n",
    "# Вложенный генератор any(c.isdigit() for c in word) проверяет каждый символ слова word и возвращает True, если хотя бы один символ — цифра.\n",
    "# Если условие истинно (то есть цифр в слове нет), слово добавляется в новый список.\n",
    "# В итоге text становится списком слов без чисел.\n",
    "# Это фильтрация списка слов, удаляющая все слова, которые содержат цифры.\n",
    "    \n",
    "# удаляем стоп-слова\n",
    "    stop = stopwords.words('english')\n",
    "# означает создание списка stop, который содержит предопределённый набор стоп-слов английского языка из библиотеки NLTK.\n",
    "# Стоп-слова — это часто используемые слова (например, артикли, предлоги, союзы: \"the\", \"is\", \"in\", \"and\"), которые обычно не несут важной смысловой нагрузки \n",
    "# при анализе текста и часто удаляются для улучшения качества и эффективности обработки.\n",
    "# Команда stopwords.words('english') загружает список таких слов на английском языке, предопределённый в NLTK.\n",
    "# Этот список используется для фильтрации текста — исключения часто встречающихся, но малоинформативных слов из анализа, например, при классификации, \n",
    "# тематическом моделировании или анализе тональности.\n",
    "    text = [x for x in text if x not in stop]\n",
    "# означает создание нового списка из элементов списка text, при этом фильтруются все слова, которые содержатся в списке стоп-слов stop.\n",
    "# То есть, из текста удаляются все слова, которые являются стоп-словами (например, предлоги, артикли, союзы), чтобы очистить текст от часто встречающихся и малоинформативных слов.\n",
    "    \n",
    "# удаляем пустые токены\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "# означает создание нового списка, включающего только те элементы из списка text, у которых длина больше нуля.\n",
    "# Проще говоря, это удаление из списка всех пустых строк или пустых элементов.\n",
    "# Такой приём часто используется при предобработке текста для удаления пустых слов (пустых строк), которые могут появляться после разбиения текста или очистки.\n",
    "    \n",
    "# pos tag text\n",
    "    pos_tags = pos_tag(text)\n",
    "# означает вызов функции pos_tag из библиотеки NLTK для разметки списка слов text частями речи.\n",
    "# Функция pos_tag принимает на вход последовательность токенов (слов) и возвращает список кортежей, где каждому слову сопоставлен его POS-тег (часть речи) согласно стандарту, например, Penn Treebank.\n",
    "# Это позволяет понять грамматическую роль каждого слова в тексте — существительное, глагол, прилагательное, наречие и другие.\n",
    "    \n",
    "# лемматизация текста\n",
    "    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "# делает лемматизацию слов из списка pos_tags.\n",
    "# Разбор компонентов:\n",
    "# pos_tags — это список кортежей (слово, POS-тег) из функции pos_tag.\n",
    "# t — само слово.\n",
    "# t — POS-тег слова (например, 'NN', 'VB' и т.д.).\n",
    "# get_wordnet_pos(t) — функция, которая преобразует POS-тег из формата Penn Treebank в формат, который понимает WordNet (например, 'N' для существительных, 'V' — для глаголов).\n",
    "# WordNetLemmatizer().lemmatize(word, pos) — метод, который преобразует слово к базовой форме (\"лемме\") с учётом части речи.\n",
    "# Итоговый список содержит лемматизированные формы всех слов.\n",
    "# Таким образом, эта конструкция выполняет лемматизацию текста, учитывая грамматическую роль каждого слова, что делает нормализацию точнее и полезнее для последующего анализа\n",
    "\n",
    "# удаляем слова из одной буквы\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "# означает создание нового списка, который содержит только те элементы из списка text, длина которых больше 1.\n",
    "# Другими словами, из списка удаляются все слова или элементы, состоящие из одного символа или пустые.\n",
    "# Это часто используется в предобработке текста, чтобы отфильтровать слишком короткие слова, которые обычно малоинформативны.\n",
    "    \n",
    "# объединяем текст\n",
    "    text = \" \".join(text)\n",
    "# означает объединение элементов списка text в одну строку, где элементы разделены пробелом.\n",
    "# Метод join вызывается на строке-разделителе (в данном случае это пробел \" \"), а в качестве аргумента принимает список строк. Он создаёт новую строку, \n",
    "# в которой все элементы из списка соединены указанным разделителем.\n",
    "    \n",
    "    return(text)\n",
    "# в Python используется внутри функции для возврата значения text обратно в вызывающий код.\n",
    "# Оператор return завершает выполнение функции и передает указанное за ним значение (в данном случае text) туда, откуда была вызвана функция. \n",
    "# Это позволяет использовать результат вычислений функции в дальнейшем в программе.\n",
    "\n",
    "# очистка текстовых данных\n",
    "\n",
    "# reviews_df[\"review_clean\"] = reviews_df[\"review\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a14cdacf-ef85-40cd-8fe5-e2b8d1be5ecc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76fc7b9-ac21-482f-b17f-1ddc31c496f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ошибка при выполнении строки\n",
    "# reviews_df[\"review_clean\"] = reviews_df[\"review\"].apply(lambda x: clean_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed1782bf-8ee1-4d79-b017-d117d12d6a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Станислав\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# https://habr.com/ru/company/Voximplant/blog/446738/\n",
    "# анализ тональности (настроения) текста отзыва\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "reviews_df[\"sentiments\"] = reviews_df[\"review\"].apply(lambda x: sid.polarity_scores(x))\n",
    "reviews_df = pd.concat([reviews_df.drop(['sentiments'], axis=1), reviews_df['sentiments'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8339d4-c680-49ed-9b4c-0100cc5acc15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
